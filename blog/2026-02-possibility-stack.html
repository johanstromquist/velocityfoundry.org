<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="The most dangerous promise in AI is 'we can make it do anything.' Not because it's false—it's increasingly true. But because it sets up a collision between infinite technical possibility and finite organizational capacity.">
    <title>The Possibility Stack: Why "We Can Do Anything" Is the Wrong Promise - Velocity Foundry</title>
    <link rel="icon" type="image/x-icon" href="../favicon.ico">
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/animations.css">
    <link rel="stylesheet" href="../assets/css/blog.css">
</head>
<body>
    <!-- Navigation (injected dynamically) -->
    <div id="blog-nav"></div>

    <!-- Post Hero (injected dynamically from posts.js) -->
    <div id="blog-hero"></div>

    <!-- Post Content -->
    <article class="container" style="padding: 0 var(--space-md) var(--space-xl);">
        <div class="post-nav" style="border-bottom: 2px solid var(--gray-200); padding-bottom: var(--space-sm); margin-bottom: var(--space-lg);" id="blog-top-nav"></div>

        <div class="post-content">

            <p>The most dangerous promise in AI right now is "we can make it do anything."</p>

            <p>Not because it's false. It's increasingly true. Two hours with Claude Code and I had seven life science data connectors in place. Five minutes later, an agent had written a desktop research piece on Trastuzumab that would have taken a medical affairs team days.</p>

            <p>The promise is dangerous because it sets up a collision: between infinite technical possibility and finite organizational capacity.</p>

            <h2>The Bespoke Trap</h2>

            <p>I've been in enough rooms now to recognize the pattern.</p>

            <p>Someone sees a demo. An agent writes a regulatory submission summary in 90 seconds. Another builds a custom dashboard from raw clinical data. A third answers questions about your own SOPs better than your QA team.</p>

            <p>The reaction is predictable: "Can it do X? What about Y? Could we make it do Z?"</p>

            <p>Yes. Probably. Almost certainly.</p>

            <p>And that's where things go wrong.</p>

            <p>Because "we can make it do anything" has a shadow side: frustration when it doesn't do <em>everything</em>. Or the expectation that it should. The same flexibility that makes bespoke AI solutions powerful makes them infinitely expandable in people's imaginations.</p>

            <p>Without boundaries, you get scope creep dressed as innovation. You get pilot projects that never end because there's always one more capability to add. You get organizational whiplash as people chase the next demonstration instead of embedding the last one.</p>

            <h2>The Possibility Stack</h2>

            <p>I've spent the last year thinking about this problem: how do we create boundaries around AI capabilities that enable controlled expansion and maintainability?</p>

            <p>The answer is a framework I call the Possibility Stack. Three tiers that determine what you can actually do with any technology:</p>

            <h3>Tier 1: Physical Possibility</h3>

            <p>What is technically possible with this technology? This is the ceiling. The raw capability. The demos that make your jaw drop.</p>

            <p>For current AI, this tier is expanding weekly. Things that were impossible six months ago are routine today. The frontier keeps moving.</p>

            <h3>Tier 2: Legal and Compliance Possibility</h3>

            <p>What is permissible? Regulatory boundaries, data governance requirements, audit trails, validation expectations.</p>

            <p>The recent FDA/EMA guidance on AI in drug development lives here. Ten principles that essentially say: the standards haven't changed, you just have new tools to meet them. GxP requirements still apply. Documentation still matters. Traceability isn't optional.</p>

            <p>This tier constrains Tier 1, but the constraints are knowable. They're written down. You can design for them.</p>

            <h3>Tier 3: Practical Possibility</h3>

            <p>What is achievable given your specific context? Your governance maturity. Your AI literacy. Your technical infrastructure. Your change capacity. Your ability to maintain what you build.</p>

            <p>This is where most organizations actually live. And it's the tier people forget to assess.</p>

            <p>You might be physically capable of deploying an autonomous agent that monitors clinical trial data in real-time. It might be legally permissible with proper validation. But if your organization isn't ready to govern it, maintain it, explain it to regulators, and integrate it into existing workflows—it's not practically possible. Not yet.</p>

            <blockquote>
                The Possibility Stack isn't about limitation. It's about clarity. Knowing the constraints across all three tiers—and the associated costs—enables expectations that are grounded, realistic, and still ambitious.
            </blockquote>

            <h2>Setting Expectations That Hold</h2>

            <p>When you understand the stack, conversations change.</p>

            <p>Instead of "Can AI do X?" the question becomes "Where does X sit in our stack?"</p>

            <p>If it's blocked at Tier 1, you're waiting for the technology. That's fine. Put it on a watch list.</p>

            <p>If it's blocked at Tier 2, you need regulatory strategy. Work with compliance to find the path, or accept that the path doesn't exist yet.</p>

            <p>If it's blocked at Tier 3—which is where most things actually stall—you have work to do. Governance frameworks. Training programs. Technical infrastructure. Change management.</p>

            <p>The third tier is uncomfortable because it's about organizational readiness, not technological capability. It's easier to blame the technology than to admit your organization isn't ready to absorb it.</p>

            <p>But that honesty is what enables progress. You can build Tier 3 readiness. You can't wish it into existence.</p>

            <h2>The Constantly Evolving Problem</h2>

            <p>Here's the objection I hear most: "But the technology changes so fast. How do we build anything durable when the ground keeps shifting?"</p>

            <p>I'm not worried about this. Not because the evolution isn't real—it is—but because there's an answer.</p>

            <p>Two things make continuous technological change manageable:</p>

            <h3>First: Platform and first-principle thinking</h3>

            <p>Don't build for specific tools. Build foundations. The agent framework we created isn't tied to any particular AI model—it's designed so skills can be added, swapped, upgraded. The investment is in the architecture, not the components.</p>

            <p>The same applies to organizational capability. Train people in foundational skills: prompt engineering principles, not specific prompt templates. Evaluation methodologies, not specific benchmarks. The ability to assess AI output, not dependence on particular AI systems.</p>

            <h3>Second: Trajectory thinking</h3>

            <p>If this capability exists now, what follows tomorrow? If I can build seven data connectors in two hours today, what does that mean for specialized research roles in eighteen months? If agents can draft regulatory documents now, what does that mean for the skills we should be developing?</p>

            <p>Trajectory thinking isn't prediction. It's preparation. You don't need to know exactly what's coming. You need to be positioned to absorb it when it arrives.</p>

            <blockquote>
                The organizations that will thrive aren't the ones that predict the future perfectly. They're the ones building the capacity to adapt to whatever future arrives.
            </blockquote>

            <h2>Litmus Tests, Not Permanent Solutions</h2>

            <p>This has implications for how you expose people to AI.</p>

            <p>The temptation is to find the perfect tool and standardize on it. To train everyone on Copilot, or Claude, or whatever platform you've chosen, and call it done.</p>

            <p>That's backwards.</p>

            <p>Tools should be litmus tests, not permanent solutions. Expose people to a variety of AI systems. Let them experience different interfaces, different strengths, different failure modes. Not because you're going to use all of them, but because the exposure builds judgment.</p>

            <p>Someone who's only ever used one AI system doesn't know what AI can do. They know what <em>that system</em> can do. The difference matters enormously when the landscape shifts—and it will shift.</p>

            <p>The goal isn't to create Copilot experts or Claude experts. It's to create people who understand AI deeply enough to evaluate whatever comes next. Tool-agnostic capability. Transferable intuition.</p>

            <h2>The Experiential Foundation</h2>

            <p>But here's the thing: none of this works in the abstract.</p>

            <p>You can't teach trajectory thinking to someone who hasn't experienced the starting point. You can't build judgment about AI capabilities without hands-on exposure. You can't create first-principle understanding from slide decks and webinars.</p>

            <p>People need to see, hear, touch, feel the technology. They need to try things, fail at things, be surprised by things. They need to build their own frame of reference—the experiential foundation upon which universal concepts can be hung.</p>

            <p>This is why I keep pushing for direct exposure over theoretical training. Let people use the tools. Let them bump into the edges. Let them discover both the magic and the limitations firsthand.</p>

            <p>The organizations moving fastest aren't the ones with the best AI strategy documents. They're the ones where people are actually using AI, learning its patterns, developing intuition for what works and what doesn't.</p>

            <p>Strategy follows experience. Not the other way around.</p>

            <h2>What This Means in Practice</h2>

            <p>If you're leading AI adoption in your organization, the Possibility Stack gives you a framework for managing expectations:</p>

            <p><strong>Map your initiatives to the stack.</strong> For each AI capability you're pursuing, identify where it's blocked. Physical? Legal? Practical? Different blockers require different responses.</p>

            <p><strong>Build Tier 3 deliberately.</strong> Governance, training, infrastructure, change capacity—these aren't obstacles to AI adoption. They're prerequisites. Invest in them.</p>

            <p><strong>Use tools as litmus tests.</strong> Expose people to variety. Build transferable skills. Don't optimize for any single platform.</p>

            <p><strong>Require experience.</strong> Before people can think strategically about AI, they need to use it. Make hands-on exposure non-negotiable.</p>

            <p><strong>Think in trajectories.</strong> Every capability you see today is a signal about tomorrow. Ask: if this now, what then?</p>

            <p>The promise of bespoke AI isn't wrong. You <em>can</em> make it do almost anything. The question is whether your organization can absorb what you build, maintain what you deploy, and adapt when the technology moves again.</p>

            <p>Constraints aren't the enemy of capability. They're the foundation of it.</p>

            <p><em>—Johan Stromquist</em></p>

        </div>

        <!-- Bottom Navigation with Next/Prev (injected dynamically) -->
        <div class="post-nav" id="blog-bottom-nav"></div>
    </article>

    <!-- Footer (injected dynamically) -->
    <div id="blog-footer"></div>

    <!-- Load posts data and layout manager -->
    <script src="posts.js"></script>
    <script src="blog-layout.js"></script>
</body>
</html>
